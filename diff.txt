diff --git a/host/include/darknet.h b/host/include/darknet.h
index 6d3d813..731623f 100755
--- a/host/include/darknet.h
+++ b/host/include/darknet.h
@@ -126,7 +126,8 @@ struct layer{
     void (*forward_TA)   (struct layer, float* net_input, int net_train);
     void (*backward_TA)   (struct layer, struct network);
     void (*update_TA)    (struct layer, update_args);
-    
+
+    int transpose; //dhkwon added in NW network
     int batch_normalize;
     int shortcut;
     int batch;
diff --git a/host/src/network.c b/host/src/network.c
index 90c77dd..20e03d5 100644
--- a/host/src/network.c
+++ b/host/src/network.c
@@ -252,8 +252,38 @@ void forward_network(network *netp)
         net.index = i;
         layer l = net.layers[i];
 
-        if(i > partition_point1 && i <= partition_point2)
-        {
+        if(i > partition_point1 && i <= partition_point2){
+
+            if(l.type == CONNECTED){ 
+                // originally last argument is adam, which is derived from params.net->adam
+                make_connected_layer_CA(l.batch, l.inputs, l.outputs, l.activation, l.batch_normalize, 0);
+            }else if(l.type == SOFTMAX){
+                make_softmax_layer_CA(l.batch, l.inputs, l.groups, l.temperature, l.w, l.h, l.c, l.spatial, l.noloss);
+            }else if(l.type == DROPOUT){
+                make_dropout_layer_CA(l.batch, l.inputs, l.probability, l.out_w, l.out_h, l.out_c, l.output, l.delta); 
+            }else if(l.type == COST){
+                make_cost_layer_CA(l.batch, l.inputs, l.cost_type, l.scale, l.ratio, l.noobject_scale, l.thresh);
+            }
+
+            if(l.type == CONNECTED){
+                int layer_TA_i = i - partition_point1 - 1;
+                transfer_weights_CA(l.biases, l.outputs, layer_TA_i, 'b', 0);
+                transfer_weights_CA(l.weights, l.outputs*l.inputs, layer_TA_i, 'w', l.transpose);
+
+                if (l.batch_normalize && (!l.dontloadscales)){
+                    transfer_weights_CA(l.scales, l.outputs, layer_TA_i, 's', 0);
+                    transfer_weights_CA(l.rolling_mean, l.outputs, layer_TA_i, 'm', 0);
+                    transfer_weights_CA(l.rolling_variance, l.outputs, layer_TA_i, 'v', 0);
+                }
+            }
+    
+            forward_network_CA(net.input, l.inputs, net.batch, net.train);
+
+            forward_network_back_CA(l.output, l.outputs, net.batch);
+
+            net.input = l.output;
+
+/*            
             // forward all the others in TEE
             if(debug_summary_com == 1){
                 summary_array("forward_network / net.input", net.input, l.inputs*net.batch);
@@ -278,7 +308,7 @@ void forward_network(network *netp)
                     summary_array("forward_network_back / l_pp2.output", l_pp2.output, l_pp2.outputs * net.batch);
                 }
             }
-
+  */          
         }else // forward in REE
         {
 
diff --git a/host/src/parser.c b/host/src/parser.c
index da6e606..f932af8 100644
--- a/host/src/parser.c
+++ b/host/src/parser.c
@@ -217,8 +217,11 @@ convolutional_layer parse_convolutional(list *options, size_params params)
     layer.dot = option_find_float_quiet(options, "dot", 0);
 
 
+    printf("parse_convolutional: %d, %d, %d, %d, %d, %d, %d, %d, %d, %d, %d, %d, %d, %d\n", 
+            batch,h,w,c,n,groups,size,stride,padding,activation, batch_normalize, binary, xnor, params.net->adam); 
+
     if(count_global > partition_point1 && count_global <= partition_point2){
-    make_convolutional_layer_CA(batch,h,w,c,n,groups,size,stride,padding,activation, batch_normalize, binary, xnor, params.net->adam, layer.flipped, layer.dot);
+//    make_convolutional_layer_CA(batch,h,w,c,n,groups,size,stride,padding,activation, batch_normalize, binary, xnor, params.net->adam, layer.flipped, layer.dot);
     }
 
     return layer;
@@ -283,9 +286,12 @@ layer parse_connected(list *options, size_params params)
 
     layer l = make_connected_layer(params.batch, params.inputs, output, activation, batch_normalize, params.net->adam);
 
+    printf("parse_conneted: %d, %d, %d, %d, %d, %d\n", 
+            params.batch, params.inputs, output, activation, batch_normalize, params.net->adam); 
+
     // send parameters into TA
     if(count_global > partition_point1 && count_global <= partition_point2){
-        make_connected_layer_CA(params.batch, params.inputs, output, activation, batch_normalize, params.net->adam);
+//        make_connected_layer_CA(params.batch, params.inputs, output, activation, batch_normalize, params.net->adam);
     }
     return l;
 }
@@ -303,8 +309,10 @@ layer parse_softmax(list *options, size_params params)
     l.spatial = option_find_float_quiet(options, "spatial", 0);
     l.noloss =  option_find_int_quiet(options, "noloss", 0);
 
+    printf("parse_softmax: %d, %d, %d, %d, %d, %d, %d, %d, %d\n",
+            params.batch, params.inputs, groups, l.temperature, l.w, l.h, l.c, l.spatial, l.noloss); 
     if(count_global > partition_point1 && count_global <= partition_point2){
-        make_softmax_layer_CA(params.batch, params.inputs, groups, l.temperature, l.w, l.h, l.c, l.spatial, l.noloss);
+//        make_softmax_layer_CA(params.batch, params.inputs, groups, l.temperature, l.w, l.h, l.c, l.spatial, l.noloss);
     }
 
     return l;
@@ -464,8 +472,11 @@ cost_layer parse_cost(list *options, size_params params)
     layer.noobject_scale =  option_find_float_quiet(options, "noobj", 1);
     layer.thresh =  option_find_float_quiet(options, "thresh",0);
 
+    printf("parse_cost: %d, %d, %d, %f, %f, %f, %f\n", 
+            params.batch, params.inputs, type, scale, layer.ratio, layer.noobject_scale, layer.thresh); 
+
     if(count_global > partition_point1 && count_global <= partition_point2){
-        make_cost_layer_CA(params.batch, params.inputs, type, scale, layer.ratio, layer.noobject_scale, layer.thresh);
+//        make_cost_layer_CA(params.batch, params.inputs, type, scale, layer.ratio, layer.noobject_scale, layer.thresh);
     }
 
     return layer;
@@ -528,8 +539,11 @@ maxpool_layer parse_maxpool(list *options, size_params params)
 
     maxpool_layer layer = make_maxpool_layer(batch,h,w,c,size,stride,padding);
 
+    printf("parse_maxpool: %d, %d, %d, %d, %d, %d, %d\n",
+        batch,h,w,c,size,stride,padding);
+
     if(count_global > partition_point1 && count_global <= partition_point2){
-        make_maxpool_layer_CA(batch,h,w,c,size,stride,padding);
+//        make_maxpool_layer_CA(batch,h,w,c,size,stride,padding);
     }
 
     return layer;
@@ -546,8 +560,12 @@ avgpool_layer parse_avgpool(list *options, size_params params)
 
     avgpool_layer layer = make_avgpool_layer(batch,w,h,c);
 
+    printf("parse_avgpool: %d, %d, %d, %d\n",
+        batch,h,w,c);
+
+
     if(count_global > partition_point1 && count_global <= partition_point2){
-        make_avgpool_layer_CA(batch,h,w,c);
+//        make_avgpool_layer_CA(batch,h,w,c);
     }
 
     return layer;
@@ -564,8 +582,11 @@ dropout_layer parse_dropout(list *options, size_params params, float *net_prev_o
     layer.output = net_prev_output;
     layer.delta = net_prev_delta;
 
+    printf("parse_dropout: %d, %d, %f, %d, %d, %d, %d, %x, %x\n",
+            params.batch, params.inputs, probability, params.w, params.h, params.c, net_prev_output, net_prev_delta); 
+
     if(count_global > partition_point1 && count_global <= partition_point2){
-        make_dropout_layer_CA(params.batch, params.inputs, probability, params.w, params.h, params.c, net_prev_output, net_prev_delta);
+//        make_dropout_layer_CA(params.batch, params.inputs, probability, params.w, params.h, params.c, net_prev_output, net_prev_delta);
     }
 
     return layer;
@@ -1345,17 +1366,19 @@ void load_connected_weights_comm(layer l, FILE *fp, int i, int transpose)
     fread(l.biases, sizeof(float), l.outputs, fp);
     fread(l.weights, sizeof(float), l.outputs*l.inputs, fp);
 
-    transfer_weights_CA(l.biases, l.outputs, i, 'b', 0);
-    transfer_weights_CA(l.weights, l.outputs*l.inputs, i, 'w', transpose);
+    // store transpose in network
+    l.transpose = transpose;
+//    transfer_weights_CA(l.biases, l.outputs, i, 'b', 0);
+//    transfer_weights_CA(l.weights, l.outputs*l.inputs, i, 'w', transpose);
 
     if (l.batch_normalize && (!l.dontloadscales)){
         fread(l.scales, sizeof(float), l.outputs, fp);
         fread(l.rolling_mean, sizeof(float), l.outputs, fp);
         fread(l.rolling_variance, sizeof(float), l.outputs, fp);
 
-        transfer_weights_CA(l.scales, l.outputs, i, 's', 0);
-        transfer_weights_CA(l.rolling_mean, l.outputs, i, 'm', 0);
-        transfer_weights_CA(l.rolling_variance, l.outputs, i, 'v', 0);
+//        transfer_weights_CA(l.scales, l.outputs, i, 's', 0);
+//        transfer_weights_CA(l.rolling_mean, l.outputs, i, 'm', 0);
+//        transfer_weights_CA(l.rolling_variance, l.outputs, i, 'v', 0);
     }
 }
 
diff --git a/ta/darknetp_ta.c b/ta/darknetp_ta.c
index 8f765f2..8eb3800 100755
--- a/ta/darknetp_ta.c
+++ b/ta/darknetp_ta.c
@@ -27,6 +27,7 @@ int debug_summary_com = 0;
 int debug_summary_pass = 0;
 int norm_output = 1;
 
+extern int layernum;
 
 void summary_array(char *print_name, float *arr, int n)
 {
@@ -558,8 +559,11 @@ static TEE_Result forward_network_back_TA_params(uint32_t param_types,
 
     float *params0 = params[0].memref.buffer;
     int buffersize = params[0].memref.size / sizeof(float);
+
+    printf("forward_network_TA_params: layernum:%d\n", layernum-1);
     for(int z=0; z<buffersize; z++){
-        params0[z] = netta.layers[netta.n-1].output[z];
+//        params0[z] = netta.layers[netta.n-1].output[z];
+        params0[z] = netta.layers[layernum-1].output[z];
     }
 
     // ?????
@@ -878,6 +882,8 @@ TEE_Result TA_InvokeCommandEntryPoint(void __maybe_unused *sess_ctx,
                                       uint32_t param_types, TEE_Param params[4])
 {
     (void)&sess_ctx; /* Unused parameter */
+    
+    printf("TA_InvokeCommandEntryPoint: %d\n", cmd_id);
 
     switch (cmd_id) {
         case MAKE_NETWORK_CMD:
@@ -911,7 +917,7 @@ TEE_Result TA_InvokeCommandEntryPoint(void __maybe_unused *sess_ctx,
         return transfer_weights_TA_params(param_types, params);
 
         case SAVE_WEI_CMD:
-            return save_weights_TA_params(param_types, params);
+        return save_weights_TA_params(param_types, params);
 
         case FORWARD_CMD:
         return forward_network_TA_params(param_types, params);
diff --git a/ta/include/user_ta_header_defines.h b/ta/include/user_ta_header_defines.h
index ed75cd2..7e97f12 100755
--- a/ta/include/user_ta_header_defines.h
+++ b/ta/include/user_ta_header_defines.h
@@ -17,7 +17,7 @@
 #define TA_STACK_SIZE			(1 * 1024 * 1024)
 
 /* Provisioned heap size for TEE_Malloc() and friends */
-#define TA_DATA_SIZE			(10 * 1024 * 1024)
+#define TA_DATA_SIZE			(5 * 1024 * 1024)
 
 /* Extra properties (give a version id and a string name) */
 #define TA_CURRENT_TA_EXT_PROPERTIES \
diff --git a/ta/network_TA.c b/ta/network_TA.c
index b1f2ba9..bed6845 100644
--- a/ta/network_TA.c
+++ b/ta/network_TA.c
@@ -13,6 +13,7 @@
 
 network_TA netta;
 int roundnum = 0;
+int layernum = 0;
 float err_sum = 0;
 float avg_loss = -1;
 
@@ -67,6 +68,55 @@ void make_network_TA(int n, float learning_rate, float momentum, float decay, in
 
 void forward_network_TA()
 {
+
+    if(roundnum == 0){
+        // ta_net_input malloc so not destroy before addition backward
+        ta_net_input = malloc(sizeof(float) * netta.layers[0].inputs * netta.layers[0].batch);
+        ta_net_delta = malloc(sizeof(float) * netta.layers[0].inputs * netta.layers[0].batch);
+
+        if(netta.workspace_size){
+            printf("workspace_size=%ld\n", netta.workspace_size);
+            netta.workspace = calloc(1, netta.workspace_size);
+        }
+    }
+    roundnum++;
+
+    netta.index = layernum;
+
+    layer_TA l = netta.layers[layernum];
+
+    if(l.delta){
+        fill_cpu_TA(l.outputs * l.batch, 0, l.delta, 1);
+    }
+
+    l.forward_TA(l, netta);
+
+    if(debug_summary_pass == 1){
+        summary_array("forward_network / l.output", l.output, l.outputs*netta.batch);
+    }
+
+//        netta.input = l.output;
+
+    if(l.truth) {
+        netta.truth = l.output;
+    }
+    //output of the network (for predict)
+    // &&
+    if(!netta.train && l.type == SOFTMAX_TA){
+        ta_net_output = malloc(sizeof(float)*l.outputs*1);
+        for(int z=0; z<l.outputs*1; z++){
+            ta_net_output[z] = l.output[z];
+        }
+    }
+
+    layernum++;
+
+    if(layernum == netta.n){    
+        calc_network_cost_TA();
+    }
+
+    /////////////////////////////////////////////////////
+/*
     if(roundnum == 0){
         // ta_net_input malloc so not destroy before addition backward
         ta_net_input = malloc(sizeof(float) * netta.layers[0].inputs * netta.layers[0].batch);
@@ -81,6 +131,7 @@ void forward_network_TA()
     roundnum++;
     int i;
     for(i = 0; i < netta.n; ++i){
+//        printf("forward_network_TA: layer num: %d\n", i);
         netta.index = i;
         layer_TA l = netta.layers[i];
 
@@ -118,6 +169,7 @@ void forward_network_TA()
     }
 
     calc_network_cost_TA();
+*/
 }
 

